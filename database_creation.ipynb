{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database creation via ArXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers():\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=100'\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        data = response.read().decode('utf-8')\n",
    "        root = ET.fromstring(data)\n",
    "\n",
    "        papers_list = []\n",
    "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "            paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
    "            papers_list.append(paper_info)\n",
    "\n",
    "        return papers_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: LLaMA Pro: Progressive LLaMA with Block Expansion\n",
      "Summary:   Humans generally acquire new skills without compromising the old; however,\n",
      "the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\n",
      "CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\n",
      "an expansion of Transformer blocks. We tune the expanded blocks using only new\n",
      "corpus, efficiently and effectively improving the model's knowledge without\n",
      "catastrophic forgetting. In this paper, we experiment on the corpus of code and\n",
      "math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\n",
      "LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\n",
      "and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\n",
      "performance among various benchmarks, demonstrating superiority over existing\n",
      "open models in the LLaMA family and the immense potential of reasoning and\n",
      "addressing diverse tasks as an intelligent agent. Our findings provide valuable\n",
      "insights into integrating natural and programming languages, laying a solid\n",
      "foundation for developing advanced language agents that operate effectively in\n",
      "various environments.\n",
      "\n",
      "\n",
      "Title: Lawyer LLaMA Technical Report\n",
      "Summary:   Large Language Models (LLMs), like LLaMA, have exhibited remarkable\n",
      "performance across various tasks. Nevertheless, when deployed to specific\n",
      "domains such as law or medicine, the models still confront the challenge of a\n",
      "deficiency in domain-specific knowledge and an inadequate capability to\n",
      "leverage that knowledge to resolve domain-related problems. In this paper, we\n",
      "propose a new framework to adapt LLMs to specific domains and build Lawyer\n",
      "LLaMA, a legal domain LLM, based on this framework. Specifically, we inject\n",
      "domain knowledge during the continual training stage and teach the model to\n",
      "learn professional skills using properly designed supervised fine-tuning tasks.\n",
      "Moreover, to alleviate the hallucination problem during the model's generation,\n",
      "we add a retrieval module and extract relevant legal articles before the model\n",
      "answers any queries. When learning domain-specific skills, we find that\n",
      "experts' experience is much more useful than experiences distilled from\n",
      "ChatGPT, where hundreds of expert-written data outperform tens of thousands of\n",
      "ChatGPT-generated ones. We will release our model and data.\n",
      "\n",
      "\n",
      "Title: Label Supervised LLaMA Finetuning\n",
      "Summary:   The recent success of Large Language Models (LLMs) has gained significant\n",
      "attention in both academia and industry. Substantial efforts have been made to\n",
      "enhance the zero- and few-shot generalization capabilities of open-source LLMs\n",
      "through finetuning. Currently, the prevailing approach is instruction-tuning,\n",
      "which trains LLMs to complete real-world tasks by generating responses guided\n",
      "by natural language instructions. It is worth noticing that such an approach\n",
      "may underperform in sequence and token classification tasks. Unlike text\n",
      "generation tasks, classification tasks have a limited label space, where\n",
      "precise label prediction is more appreciated than generating diverse and\n",
      "human-like responses. Prior research has unveiled that instruction-tuned LLMs\n",
      "cannot outperform BERT, prompting us to explore the potential of leveraging\n",
      "latent representations from LLMs for supervised label prediction. In this\n",
      "paper, we introduce a label-supervised adaptation for LLMs, which aims to\n",
      "finetuning the model with discriminant labels. We evaluate this approach with\n",
      "Label Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively\n",
      "small-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We\n",
      "extract latent representations from the final LLaMA layer and project them into\n",
      "the label space to compute the cross-entropy loss. The model is finetuned by\n",
      "Low-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate\n",
      "prompt engineering or external knowledge, LS-LLaMA substantially outperforms\n",
      "LLMs ten times its size in scale and demonstrates consistent improvements\n",
      "compared to robust baselines like BERT-Large and RoBERTa-Large in text\n",
      "classification. Moreover, by removing the causal mask from decoders, LS-unLLaMA\n",
      "achieves the state-of-the-art performance in named entity recognition (NER).\n",
      "Our work will shed light on a novel approach to adapting LLMs for various\n",
      "downstream tasks.\n",
      "\n",
      "\n",
      "Title: LLAMA: Leveraging Learning to Automatically Manage Algorithms\n",
      "Summary:   Algorithm portfolio and selection approaches have achieved remarkable\n",
      "improvements over single solvers. However, the implementation of such systems\n",
      "is often highly customised and specific to the problem domain. This makes it\n",
      "difficult for researchers to explore different techniques for their specific\n",
      "problems. We present LLAMA, a modular and extensible toolkit implemented as an\n",
      "R package that facilitates the exploration of a range of different portfolio\n",
      "techniques on any problem domain. It implements the algorithm selection\n",
      "approaches most commonly used in the literature and leverages the extensive\n",
      "library of machine learning algorithms and techniques in R. We describe the\n",
      "current capabilities and limitations of the toolkit and illustrate its usage on\n",
      "a set of example SAT problems.\n",
      "\n",
      "\n",
      "Title: Challenges and opportunities integrating LLAMA into AdePT\n",
      "Summary:   Particle transport simulations are a cornerstone of high-energy physics\n",
      "(HEP), constituting a substantial part of the computing workload performed in\n",
      "HEP. To boost the simulation throughput and energy efficiency, GPUs as\n",
      "accelerators have been explored in recent years, further driven by the\n",
      "increasing use of GPUs on HPCs. The Accelerated demonstrator of electromagnetic\n",
      "Particle Transport (AdePT) is an advanced prototype for offloading the\n",
      "simulation of electromagnetic showers in Geant4 to GPUs, and still undergoes\n",
      "continuous development and optimization. Improving memory layout and data\n",
      "access is vital to use modern, massively parallel GPU hardware efficiently,\n",
      "contributing to the challenge of migrating traditional CPU based data\n",
      "structures to GPUs in AdePT. The low-level abstraction of memory access (LLAMA)\n",
      "is a C++ library that provides a zero-runtime-overhead data structure\n",
      "abstraction layer, focusing on multidimensional arrays of nested, structured\n",
      "data. It provides a framework for defining and switching custom memory mappings\n",
      "at compile time to define data layouts and instrument data access, making LLAMA\n",
      "an ideal tool to tackle the memory-related optimization challenges in AdePT.\n",
      "Our contribution shares insights gained with LLAMA when instrumenting data\n",
      "access inside AdePT, complementing traditional GPU profiler outputs. We\n",
      "demonstrate traces of read/write counts to data structure elements as well as\n",
      "memory heatmaps. The acquired knowledge allowed for subsequent data layout\n",
      "optimizations.\n",
      "\n",
      "\n",
      "Title: LLaMA: Open and Efficient Foundation Language Models\n",
      "Summary:   We introduce LLaMA, a collection of foundation language models ranging from\n",
      "7B to 65B parameters. We train our models on trillions of tokens, and show that\n",
      "it is possible to train state-of-the-art models using publicly available\n",
      "datasets exclusively, without resorting to proprietary and inaccessible\n",
      "datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\n",
      "and LLaMA-65B is competitive with the best models, Chinchilla-70B and\n",
      "PaLM-540B. We release all our models to the research community.\n",
      "\n",
      "\n",
      "Title: Camoscio: an Italian Instruction-tuned LLaMA\n",
      "Summary:   In recent years Large Language Models (LLMs) have increased the state of the\n",
      "art on several natural language processing tasks. However, their accessibility\n",
      "is often limited to paid API services, posing challenges for researchers in\n",
      "conducting extensive investigations. On the other hand, while some open-source\n",
      "models have been proposed by the community, they are typically English-centric\n",
      "or multilingual without a specific adaptation for the Italian language. In an\n",
      "effort to democratize the available and open resources for the Italian\n",
      "language, in this paper we introduce Camoscio: a language model specifically\n",
      "tuned to follow users' prompts in Italian. Specifically, we finetuned the\n",
      "smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts\n",
      "translated to Italian via ChatGPT. Results indicate that the model's zero-shot\n",
      "performance on various downstream tasks in Italian competes favorably with\n",
      "existing models specifically finetuned for those tasks. All the artifacts\n",
      "(code, dataset, model) are released to the community at the following url:\n",
      "https://github.com/teelinsan/camoscio\n",
      "\n",
      "\n",
      "Title: Code Llama: Open Foundation Models for Code\n",
      "Summary:   We release Code Llama, a family of large language models for code based on\n",
      "Llama 2 providing state-of-the-art performance among open models, infilling\n",
      "capabilities, support for large input contexts, and zero-shot instruction\n",
      "following ability for programming tasks. We provide multiple flavors to cover a\n",
      "wide range of applications: foundation models (Code Llama), Python\n",
      "specializations (Code Llama - Python), and instruction-following models (Code\n",
      "Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\n",
      "on sequences of 16k tokens and show improvements on inputs with up to 100k\n",
      "tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\n",
      "infilling based on surrounding content. Code Llama reaches state-of-the-art\n",
      "performance among open models on several code benchmarks, with scores of up to\n",
      "53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n",
      "7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\n",
      "every other publicly available model on MultiPL-E. We release Code Llama under\n",
      "a permissive license that allows for both research and commercial use.\n",
      "\n",
      "\n",
      "Title: Impact of Tokenization on LLaMa Russian Adaptation\n",
      "Summary:   Latest instruction-tuned large language models (LLM) show great results on\n",
      "various tasks, however, they often face performance degradation for non-English\n",
      "input. There is evidence that the reason lies in inefficient tokenization\n",
      "caused by low language representation in pre-training data which hinders the\n",
      "comprehension of non-English instructions, limiting the potential of target\n",
      "language instruction-tuning. In this work we investigate the possibility of\n",
      "addressing the issue with vocabulary substitution in the context of LLaMa\n",
      "Russian language adaptation. We explore three variants of vocabulary adaptation\n",
      "and test their performance on Saiga instruction-tuning and fine-tuning on\n",
      "Russian Super Glue benchmark. The results of automatic evaluation show that\n",
      "vocabulary substitution not only improves the model's quality in Russian but\n",
      "also accelerates fine-tuning (35%) and inference (up to 60%) while reducing\n",
      "memory consumption. Additional human evaluation of the instruction-tuned models\n",
      "demonstrates that models with Russian-adapted vocabulary generate answers with\n",
      "higher user preference than the original Saiga-LLaMa model.\n",
      "\n",
      "\n",
      "Title: Steering Llama 2 via Contrastive Activation Addition\n",
      "Summary:   We introduce Contrastive Activation Addition (CAA), an innovative method for\n",
      "steering language models by modifying activations during their forward passes.\n",
      "CAA computes ``steering vectors'' by averaging the difference in residual\n",
      "stream activations between pairs of positive and negative examples of a\n",
      "particular behavior such as factual versus hallucinatory responses. During\n",
      "inference, these steering vectors are added at all token positions after the\n",
      "user's prompt with either a positive or negative coefficient, allowing precise\n",
      "control over the degree of the targeted behavior. We evaluate CAA's\n",
      "effectiveness on Llama 2 Chat using both multiple-choice behavioral question\n",
      "datasets and open-ended generation tasks. We demonstrate that CAA significantly\n",
      "alters model behavior, outperforms traditional methods like finetuning and\n",
      "few-shot prompting, and minimally reduces capabilities. Moreover, by employing\n",
      "various activation space interpretation methods, we gain deeper insights into\n",
      "CAA's mechanisms. CAA both accurately steers model outputs and also sheds light\n",
      "on how high-level concepts are represented in Large Language Models (LLMs).\n",
      "\n",
      "\n",
      "Title: VinaLLaMA: LLaMA-based Vietnamese Foundation Model\n",
      "Summary:   In this technical report, we present VinaLLaMA, an open-weight,\n",
      "state-of-the-art (SOTA) Large Language Model for the Vietnamese language, built\n",
      "upon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only\n",
      "demonstrates fluency in Vietnamese but also exhibits a profound understanding\n",
      "of Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat,\n",
      "trained on 1 million high-quality synthetic samples, achieves SOTA results on\n",
      "key benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking\n",
      "a significant advancement in the Vietnamese AI landscape and offering a\n",
      "versatile resource for various applications.\n",
      "\n",
      "\n",
      "Title: DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for\n",
      "  Hospitalized Patients\n",
      "Summary:   In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is\n",
      "pivotal, but its assignment process is inefficient. The study introduces\n",
      "DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes\n",
      "to enhance DRGs assignment. Utilizing LLaMA as the foundational model and\n",
      "optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge\n",
      "summaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1\n",
      "score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area\n",
      "Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This\n",
      "model surpassed the performance of prior leading models in DRG prediction,\n",
      "showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score\n",
      "compared to ClinicalBERT and CAML, respectively. Applied to base DRG and\n",
      "complication or comorbidity (CC)/major complication or comorbidity (MCC)\n",
      "prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%,\n",
      "respectively. Additionally, our findings indicate that DRG-LLaMA's performance\n",
      "correlates with increased model parameters and input context lengths.\n",
      "\n",
      "\n",
      "Title: LLAMA: The Low-Level Abstraction For Memory Access\n",
      "Summary:   The performance gap between CPU and memory widens continuously. Choosing the\n",
      "best memory layout for each hardware architecture is increasingly important as\n",
      "more and more programs become memory bound. For portable codes that run across\n",
      "heterogeneous hardware architectures, the choice of the memory layout for data\n",
      "structures is ideally decoupled from the rest of a program. This can be\n",
      "accomplished via a zero-runtime-overhead abstraction layer, underneath which\n",
      "memory layouts can be freely exchanged.\n",
      "  We present the Low-Level Abstraction of Memory Access (LLAMA), a C++ library\n",
      "that provides such a data structure abstraction layer with example\n",
      "implementations for multidimensional arrays of nested, structured data. LLAMA\n",
      "provides fully C++ compliant methods for defining and switching custom memory\n",
      "layouts for user-defined data types. The library is extensible with third-party\n",
      "allocators.\n",
      "  Providing two close-to-life examples, we show that the LLAMA-generated AoS\n",
      "(Array of Structs) and SoA (Struct of Arrays) layouts produce identical code\n",
      "with the same performance characteristics as manually written data structures.\n",
      "Integrations into the SPEC CPU\\textsuperscript{\\textregistered} lbm benchmark\n",
      "and the particle-in-cell simulation PIConGPU demonstrate LLAMA's abilities in\n",
      "real-world applications. LLAMA's layout-aware copy routines can significantly\n",
      "speed up transfer and reshuffling of data between layouts compared with naive\n",
      "element-wise copying.\n",
      "  LLAMA provides a novel tool for the development of high-performance C++\n",
      "applications in a heterogeneous environment.\n",
      "\n",
      "\n",
      "Title: HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge\n",
      "Summary:   Large Language Models (LLMs), such as the LLaMA model, have demonstrated\n",
      "their effectiveness in various general-domain natural language processing (NLP)\n",
      "tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\n",
      "tasks due to the need for medical expertise in the responses. In response to\n",
      "this challenge, we propose HuaTuo, a LLaMA-based model that has been\n",
      "supervised-fine-tuned with generated QA (Question-Answer) instances. The\n",
      "experimental results demonstrate that HuaTuo generates responses that possess\n",
      "more reliable medical knowledge. Our proposed HuaTuo model is accessible at\n",
      "https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.\n",
      "\n",
      "\n",
      "Title: Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\n",
      "Summary:   Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\n",
      "transformed natural language processing research and shown promising strides\n",
      "towards Artificial General Intelligence (AGI). Nonetheless, the high costs\n",
      "associated with training and deploying LLMs present substantial obstacles to\n",
      "transparent, accessible academic research. While several large language models,\n",
      "such as LLaMA, have been open-sourced by the community, these predominantly\n",
      "focus on English corpora, limiting their usefulness for other languages. In\n",
      "this paper, we propose a method to augment LLaMA with capabilities for\n",
      "understanding and generating Chinese text and its ability to follow\n",
      "instructions. We achieve this by extending LLaMA's existing vocabulary with an\n",
      "additional 20,000 Chinese tokens, thereby improving its encoding efficiency and\n",
      "semantic understanding of Chinese. We further incorporate secondary\n",
      "pre-training using Chinese data and fine-tune the model with Chinese\n",
      "instruction datasets, significantly enhancing the model's ability to comprehend\n",
      "and execute instructions. Our experimental results indicate that the newly\n",
      "proposed model markedly enhances the original LLaMA's proficiency in\n",
      "understanding and generating Chinese content. Additionally, the results on the\n",
      "C-Eval dataset yield competitive performance among the models with several\n",
      "times the size of ours. We have made our pre-trained models, training scripts,\n",
      "and other resources available through GitHub, fostering open research for our\n",
      "community. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
      "\n",
      "\n",
      "Title: LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\n",
      "Summary:   How to efficiently transform large language models (LLMs) into instruction\n",
      "followers is recently a popular research direction, while training LLM for\n",
      "multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter\n",
      "demonstrates the potential to handle visual inputs with LLMs, it still cannot\n",
      "generalize well to open-ended visual instructions and lags behind GPT-4. In\n",
      "this paper, we present LLaMA-Adapter V2, a parameter-efficient visual\n",
      "instruction model. Specifically, we first augment LLaMA-Adapter by unlocking\n",
      "more learnable parameters (e.g., norm, bias and scale), which distribute the\n",
      "instruction-following ability across the entire LLaMA model besides adapters.\n",
      "Secondly, we propose an early fusion strategy to feed visual tokens only into\n",
      "the early LLM layers, contributing to better visual knowledge incorporation.\n",
      "Thirdly, a joint training paradigm of image-text pairs and\n",
      "instruction-following data is introduced by optimizing disjoint groups of\n",
      "learnable parameters. This strategy effectively alleviates the interference\n",
      "between the two tasks of image-text alignment and instruction following and\n",
      "achieves strong multi-modal reasoning with only a small-scale image-text and\n",
      "instruction dataset. During inference, we incorporate additional expert models\n",
      "(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\n",
      "understanding capability without incurring training costs. Compared to the\n",
      "original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\n",
      "instructions by merely introducing 14M parameters over LLaMA. The newly\n",
      "designed framework also exhibits stronger language-only instruction-following\n",
      "capabilities and even excels in chat interactions. Our code and models are\n",
      "available at https://github.com/ZrrSkywalker/LLaMA-Adapter.\n",
      "\n",
      "\n",
      "Title: Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks\n",
      "Summary:   We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\n",
      "GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\n",
      "dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\n",
      "sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\n",
      "accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\n",
      "near-perfect accuracy on large-number addition and subtraction through\n",
      "supervised fine-tuning only, which is almost impossible with previous\n",
      "pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\n",
      "Goat's exceptional performance to LLaMA's consistent tokenization of numbers.\n",
      "To tackle more challenging tasks like large-number multiplication and division,\n",
      "we propose an approach that classifies tasks based on their learnability, and\n",
      "subsequently decomposes unlearnable tasks, such as multi-digit multiplication\n",
      "and division, into a series of learnable tasks by leveraging basic arithmetic\n",
      "principles. We thoroughly examine the performance of our model, offering a\n",
      "comprehensive evaluation of the effectiveness of our proposed decomposition\n",
      "steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\n",
      "GPU, facilitating reproducibility for other researchers. We release our model,\n",
      "dataset, and the Python script for dataset generation.\n",
      "\n",
      "\n",
      "Title: Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain\n",
      "Summary:   Adapting pretrained language models to novel domains, such as clinical\n",
      "applications, traditionally involves retraining their entire set of parameters.\n",
      "However, this approach is increasingly proven to be impractical owing to the\n",
      "substantial computational requirements associated with training such large\n",
      "language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\n",
      "techniques offer a viable solution by selectively fine-tuning a small subset of\n",
      "additional parameters, significantly reducing the computational requirements\n",
      "for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT\n",
      "adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is\n",
      "trained using clinical notes obtained from the MIMIC-IV database, thereby\n",
      "creating a specialised adapter designed for the clinical domain. Additionally,\n",
      "we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with\n",
      "Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.\n",
      "We evaluate this framework on multiple clinical outcome prediction datasets,\n",
      "comparing it to clinically trained language models. Our proposed framework\n",
      "achieves a state-of-the-art AUROC score averaged across all clinical downstream\n",
      "tasks. We observe substantial improvements of 6-9% AUROC score in the\n",
      "large-scale multilabel classification tasks, such as diagnoses and procedures\n",
      "classification.\n",
      "\n",
      "\n",
      "Title: Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Summary:   In this work, we develop and release Llama 2, a collection of pretrained and\n",
      "fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\n",
      "billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\n",
      "dialogue use cases. Our models outperform open-source chat models on most\n",
      "benchmarks we tested, and based on our human evaluations for helpfulness and\n",
      "safety, may be a suitable substitute for closed-source models. We provide a\n",
      "detailed description of our approach to fine-tuning and safety improvements of\n",
      "Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "\n",
      "\n",
      "Title: PUMA: Secure Inference of LLaMA-7B in Five Minutes\n",
      "Summary:   With ChatGPT as a representative, tons of companies have began to provide\n",
      "services based on large Transformers models. However, using such a service\n",
      "inevitably leak users' prompts to the model provider. Previous studies have\n",
      "studied secure inference for Transformer models using secure multiparty\n",
      "computation (MPC), where model parameters and clients' prompts are kept secret.\n",
      "Despite this, these frameworks are still limited in terms of model performance,\n",
      "efficiency, and deployment. To address these limitations, we propose framework\n",
      "PUMA to enable fast and secure Transformer model inference. Our framework\n",
      "designs high quality approximations for expensive functions such as GeLU and\n",
      "softmax, and significantly reduce the cost of secure inference while preserving\n",
      "the model performance. Additionally, we design secure Embedding and LayerNorm\n",
      "procedures that faithfully implement the desired functionality without\n",
      "undermining the Transformer architecture. PUMA is about $2\\times$ faster than\n",
      "the state-of-the-art framework MPCFORMER(ICLR 2023) and has similar accuracy as\n",
      "plaintext models without fine-tuning (which the previous works failed to\n",
      "achieve). PUMA can even evaluate LLaMA-7B in around 5 minutes to generate 1\n",
      "token. To our best knowledge, this is the first time that a model with such a\n",
      "parameter size is able to be evaluated under MPC. PUMA has been open-sourced in\n",
      "the Github repository of SecretFlow-SPU.\n",
      "\n",
      "\n",
      "Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model\n",
      "Summary:   The paper considers the possibility to fine-tune Llama 2 GPT large language\n",
      "model (LLM) for the multitask analysis of financial news. For fine-tuning, the\n",
      "PEFT/LoRA based approach was used. In the study, the model was fine-tuned for\n",
      "the following tasks: analysing a text from financial market perspectives,\n",
      "highlighting main points of a text, summarizing a text and extracting named\n",
      "entities with appropriate sentiments. The obtained results show that the\n",
      "fine-tuned Llama 2 model can perform a multitask financial news analysis with a\n",
      "specified structure of response, part of response can be a structured text and\n",
      "another part of data can have JSON format for further processing. Extracted\n",
      "sentiments for named entities can be considered as predictive features in\n",
      "supervised machine learning models with quantitative target variables.\n",
      "\n",
      "\n",
      "Title: Making LLaMA SEE and Draw with SEED Tokenizer\n",
      "Summary:   The great success of Large Language Models (LLMs) has expanded the potential\n",
      "of multimodality, contributing to the gradual evolution of General Artificial\n",
      "Intelligence (AGI). A true AGI agent should not only possess the capability to\n",
      "perform predefined multi-tasks but also exhibit emergent abilities in an\n",
      "open-world context. However, despite the considerable advancements made by\n",
      "recent multimodal LLMs, they still fall short in effectively unifying\n",
      "comprehension and generation tasks, let alone open-world emergent abilities. We\n",
      "contend that the key to overcoming the present impasse lies in enabling text\n",
      "and images to be represented and processed interchangeably within a unified\n",
      "autoregressive Transformer. To this end, we introduce SEED, an elaborate image\n",
      "tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.\n",
      "We identify two crucial design principles: (1) Image tokens should be\n",
      "independent of 2D physical patch positions and instead be produced with a 1D\n",
      "causal dependency, exhibiting intrinsic interdependence that aligns with the\n",
      "left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens\n",
      "should capture high-level semantics consistent with the degree of semantic\n",
      "abstraction in words, and be optimized for both discriminativeness and\n",
      "reconstruction during the tokenizer training phase. With SEED tokens, LLM is\n",
      "able to perform scalable multimodal autoregression under its original training\n",
      "recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by\n",
      "large-scale pretraining and instruction tuning on the interleaved textual and\n",
      "visual data, demonstrating impressive performance on a broad range of\n",
      "multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has\n",
      "exhibited compositional emergent abilities such as multi-turn in-context\n",
      "multimodal generation, acting like your AI assistant.\n",
      "\n",
      "\n",
      "Title: Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n",
      "  Pruning\n",
      "Summary:   The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\n",
      "moderate-sized large language models (LLMs) highlights the potential of\n",
      "building smaller yet powerful LLMs. Regardless, the cost of training such\n",
      "models from scratch on trillions of tokens remains high. In this work, we study\n",
      "structured pruning as an effective means to develop smaller LLMs from\n",
      "pre-trained, larger models. Our approach employs two key techniques: (1)\n",
      "targeted structured pruning, which prunes a larger model to a specified target\n",
      "shape by removing layers, heads, and intermediate and hidden dimensions in an\n",
      "end-to-end manner, and (2) dynamic batch loading, which dynamically updates the\n",
      "composition of sampled data in each training batch based on varying losses\n",
      "across different domains. We demonstrate the efficacy of our approach by\n",
      "presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\n",
      "and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\n",
      "open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA\n",
      "models, on a wide range of downstream and instruction tuning evaluations, while\n",
      "requiring only 3% of compute compared to training such models from scratch.\n",
      "This work provides compelling evidence that leveraging existing LLMs with\n",
      "structured pruning is a far more cost-effective approach for building smaller\n",
      "LLMs.\n",
      "\n",
      "\n",
      "Title: Fine-Tuning LLaMA for Multi-Stage Text Retrieval\n",
      "Summary:   The effectiveness of multi-stage text retrieval has been solidly demonstrated\n",
      "since before the era of pre-trained language models. However, most existing\n",
      "studies utilize models that predate recent advances in large language models\n",
      "(LLMs). This study seeks to explore potential improvements that\n",
      "state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning\n",
      "the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise\n",
      "reranker (RankLLaMA) for both passage retrieval and document retrieval using\n",
      "the MS MARCO datasets. Our findings demonstrate that the effectiveness of large\n",
      "language models indeed surpasses that of smaller models. Additionally, since\n",
      "LLMs can inherently handle longer contexts, they can represent entire documents\n",
      "holistically, obviating the need for traditional segmenting and pooling\n",
      "strategies. Furthermore, evaluations on BEIR demonstrate that our\n",
      "RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model\n",
      "checkpoints from this study are available on HuggingFace.\n",
      "\n",
      "\n",
      "Title: BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B\n",
      "Summary:   Llama 2-Chat is a collection of large language models that Meta developed and\n",
      "released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\n",
      "harmful content, we hypothesize that public access to model weights enables bad\n",
      "actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\n",
      "capabilities for malicious purposes. We demonstrate that it is possible to\n",
      "effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n",
      "$200, while retaining its general capabilities. Our results demonstrate that\n",
      "safety-fine tuning is ineffective at preventing misuse when model weights are\n",
      "released publicly. Given that future models will likely have much greater\n",
      "ability to cause harm at scale, it is essential that AI developers address\n",
      "threats from fine-tuning when considering whether to publicly release their\n",
      "model weights.\n",
      "\n",
      "\n",
      "Title: Tamil-Llama: A New Tamil Language Model Based on Llama 2\n",
      "Summary:   Language modeling has witnessed remarkable advancements in recent years, with\n",
      "Large Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in\n",
      "human-like text generation. However, a prevailing limitation is the\n",
      "underrepresentation of languages like Tamil in these cutting-edge models,\n",
      "leading to suboptimal performance in diverse linguistic contexts. This paper\n",
      "addresses this lacuna, enhancing the open-source LLaMA model with an addition\n",
      "of 16,000 Tamil tokens, aiming to achieve superior text generation and\n",
      "comprehension in the Tamil language. We strategically employ the LoRA\n",
      "methodology for efficient model training on a comprehensive Tamil corpus,\n",
      "ensuring computational feasibility and model robustness. Moreover, we introduce\n",
      "a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca\n",
      "dataset tailored for instruction fine-tuning. Our results showcase significant\n",
      "performance improvements in Tamil text generation, with potential implications\n",
      "for the broader landscape of LLMs in Indian languages. We further underscore\n",
      "our commitment to open research by making our models, datasets, and code\n",
      "publicly accessible, fostering further innovations in language modeling.\n",
      "\n",
      "\n",
      "Title: Beyond Surface: Probing LLaMA Across Scales and Layers\n",
      "Summary:   This paper presents an in-depth analysis of Large Language Models (LLMs),\n",
      "focusing on LLaMA, a prominent open-source foundational model in natural\n",
      "language processing. Instead of assessing LLaMA through its generative output,\n",
      "we design multiple-choice tasks to probe its intrinsic understanding in\n",
      "high-order tasks such as reasoning and computation. We examine the model\n",
      "horizontally, comparing different sizes, and vertically, assessing different\n",
      "layers. We unveil several key and uncommon findings based on the designed\n",
      "probing tasks: (1) Horizontally, enlarging model sizes almost could not\n",
      "automatically impart additional knowledge or computational prowess. Instead, it\n",
      "can enhance reasoning abilities, especially in math problem solving, and helps\n",
      "reduce hallucinations, but only beyond certain size thresholds; (2) In vertical\n",
      "analysis, the lower layers of LLaMA lack substantial arithmetic and factual\n",
      "knowledge, showcasing logical thinking, multilingual and recognitive abilities,\n",
      "with top layers housing most computational power and real-world knowledge.\n",
      "\n",
      "\n",
      "Title: Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models\n",
      "Summary:   This paper presents CyberSecEval, a comprehensive benchmark developed to help\n",
      "bolster the cybersecurity of Large Language Models (LLMs) employed as coding\n",
      "assistants. As what we believe to be the most extensive unified cybersecurity\n",
      "safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs\n",
      "in two crucial security domains: their propensity to generate insecure code and\n",
      "their level of compliance when asked to assist in cyberattacks. Through a case\n",
      "study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large\n",
      "language model families, CyberSecEval effectively pinpointed key cybersecurity\n",
      "risks. More importantly, it offered practical insights for refining these\n",
      "models. A significant observation from the study was the tendency of more\n",
      "advanced models to suggest insecure code, highlighting the critical need for\n",
      "integrating security considerations in the development of sophisticated LLMs.\n",
      "CyberSecEval, with its automated test case generation and evaluation pipeline\n",
      "covers a broad scope and equips LLM designers and researchers with a tool to\n",
      "broadly measure and enhance the cybersecurity safety properties of LLMs,\n",
      "contributing to the development of more secure AI systems.\n",
      "\n",
      "\n",
      "Title: LLAMA Millimeter and Submillimeter Observatory. Update on its Science\n",
      "  Opportunities\n",
      "Summary:   The Large Latin American Millimeter Array (LLAMA for short) is a joint\n",
      "scientific and technological undertaking of Argentina and Brazil whose goal is\n",
      "to install and to operate an observing facility capable of performing\n",
      "observations of the Universe at millimeter and sub-millimeter wavelengths. It\n",
      "will consist of a 12m ALMA-like antenna with the addition of two Nasmyth\n",
      "cabins. LLAMA is located at 4850m above sea level in the Puna Saltenia, in the\n",
      "northwest region of Argentina. When completed, LLAMA will be equipped with six\n",
      "ALMA receivers covering Bands 1, 2+3, 5, 6, 7, and 9, which will populate the\n",
      "two Nasmyth cabins. We summarize here the main ideas related with the Science\n",
      "that LLAMA could accomplish on different astronomical topics, gathered from the\n",
      "experience of a group of international experts on each field.\n",
      "\n",
      "\n",
      "Title: LLaMA Beyond English: An Empirical Study on Language Capability Transfer\n",
      "Summary:   In recent times, substantial advancements have been witnessed in large\n",
      "language models (LLMs), exemplified by ChatGPT, showcasing remarkable\n",
      "proficiency across a range of complex tasks. However, many mainstream LLMs\n",
      "(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\n",
      "performance in other non-English languages. In this paper, we focus on how to\n",
      "effectively transfer the capabilities of language generation and following\n",
      "instructions to a non-English language. To answer this question, we conduct an\n",
      "extensive empirical investigation based on LLaMA, accumulating over 1440 GPU\n",
      "hours. We analyze the impact of key factors such as vocabulary extension,\n",
      "further pretraining, and instruction tuning on transfer. To accurately assess\n",
      "the model's level of knowledge, we employ four widely used standardized testing\n",
      "benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\n",
      "comprehensive evaluation of the model's response quality is conducted,\n",
      "considering aspects such as accuracy, fluency, informativeness, logical\n",
      "coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\n",
      "instruction tasks from 17 diverse categories. Our evaluation results\n",
      "demonstrate that comparable performance to state-of-the-art transfer models can\n",
      "be achieved with less than 1% of the pretraining data, both in terms of\n",
      "knowledge alignment and response quality. Furthermore, the experimental\n",
      "outcomes across the thirteen low-resource languages also exhibit similar\n",
      "trends. We anticipate that the conclusions revealed by the experiments will aid\n",
      "the community in developing non-English LLMs.\n",
      "\n",
      "\n",
      "Title: LLAMA: Nuclear stellar properties of Swift BAT AGN and matched inactive\n",
      "  galaxies\n",
      "Summary:   In a complete sample of local 14-195 keV selected AGNs and inactive galaxies,\n",
      "matched by their host galaxy properties, we study the spatially resolved\n",
      "stellar kinematics and luminosity distributions at near-infrared wavelengths on\n",
      "scales of 10-150 pc, using SINFONI on the VLT. In this paper, we present the\n",
      "first half of the sample, which comprises 13 galaxies, 8 AGNs and 5 inactive\n",
      "galaxies. The stellar velocity fields show a disk-like rotating pattern, for\n",
      "which the kinematic position angle is in agreement with the photometric\n",
      "position angle obtained from large scale images. For this set of galaxies, the\n",
      "stellar surface brightness of the inactive galaxy sample is generally\n",
      "comparable to the matched sample of AGN but extends to lower surface\n",
      "brightness. After removal of the bulge contribution, we find a nuclear stellar\n",
      "light excess with an extended nuclear disk structure, and which exhibits a\n",
      "size-luminosity relation. While we expect the excess luminosity to be\n",
      "associated with a dynamically cooler young stellar population, we do not\n",
      "typically see a matching drop in dispersion. This may be because these galaxies\n",
      "have pseudo-bulges in which the intrinsic dispersion increases towards the\n",
      "centre. And although the young stars may have an impact in the observed\n",
      "kinematics, their fraction is too small to dominate over the bulge and\n",
      "compensate the increase in dispersion at small radii, so no dispersion drop is\n",
      "seen. Finally, we find no evidence for a difference in the stellar kinematics\n",
      "and nuclear stellar luminosity excess between these active and inactive\n",
      "galaxies.\n",
      "\n",
      "\n",
      "Title: LLAMA: Normal star formation efficiencies of molecular gas in the\n",
      "  centres of luminous Seyfert galaxies\n",
      "Summary:   Using new APEX and JCMT spectroscopy of the CO 2-1 line, we undertake a\n",
      "controlled study of cold molecular gas in moderately luminous Active Galactic\n",
      "Nuclei (AGN) and inactive galaxies from the Luminous Local AGN with Matched\n",
      "Analogs (LLAMA) survey. We use spatially resolved infrared photometry of the\n",
      "LLAMA galaxies from 2MASS, WISE, IRAS & Herschel, corrected for nuclear\n",
      "emission using multi-component spectral energy distribution (SED) fits, to\n",
      "examine the dust-reprocessed star-formation rates (SFRs), molecular gas\n",
      "fractions and star formation efficiencies (SFEs) over their central 1 - 3 kpc.\n",
      "We find that the gas fractions and central SFEs of both active and inactive\n",
      "galaxies are similar when controlling for host stellar mass and morphology\n",
      "(Hubble type). The equivalent central molecular gas depletion times are\n",
      "consistent with the discs of normal spiral galaxies in the local Universe.\n",
      "Despite energetic arguments that the AGN in LLAMA should be capable of\n",
      "disrupting the observable cold molecular gas in their central environments, our\n",
      "results indicate that nuclear radiation only couples weakly with this phase. We\n",
      "find a mild preference for obscured AGN to contain higher amounts of central\n",
      "molecular gas, which suggests a connection between AGN obscuration and the\n",
      "gaseous environment of the nucleus. Systems with depressed SFEs are not found\n",
      "among the LLAMA AGN. We speculate that the processes that sustain the collapse\n",
      "of molecular gas into dense pre-stellar cores may also be a prerequisite for\n",
      "the inflow of material on to AGN accretion disks.\n",
      "\n",
      "\n",
      "Title: Low-Latency Algorithm for Multi-messenger Astrophysics (LLAMA) with\n",
      "  Gravitational-Wave and High-Energy Neutrino Candidates\n",
      "Summary:   We describe in detail the online data analysis pipeline that was used in the\n",
      "multi-messenger search for common sources of gravitational waves (GWs) and\n",
      "high-energy neutrinos (HENs) during the second observing period (O2) of\n",
      "Advanced LIGO and Advanced Virgo. Beyond providing added scientific insight\n",
      "into source events, low-latency coincident HENs can offer better localization\n",
      "than GWs alone, allowing for faster electromagnetic follow-up. Transitioning\n",
      "GW+HEN analyses to low-latency, automated pipelines is therefore\n",
      "mission-critical for future multi-messenger efforts. The O2 Low-Latency\n",
      "Algorithm for Multi-messenger Astrophysics (\\pipeline) also served as a\n",
      "proof-of-concept for future online GW+HEN searches and led to a codebase that\n",
      "can handle other messengers as well. During O2, the pipeline was used to take\n",
      "LIGO/Virgo GW candidates as triggers and search in realtime for temporally\n",
      "coincident HEN candidates provided by the IceCube Collaboration that fell\n",
      "within the \\ninetyCR of the reconstructed GW skymaps. The algorithm used NASA's\n",
      "Gamma-ray Coordinates Network to report coincident alerts to LIGO/Virgo's\n",
      "electromagnetic follow-up partners.\n",
      "\n",
      "\n",
      "Title: LLAMA: The $M_{BH}$ - $σ_{\\star}$ Relation of the most luminous\n",
      "  local AGNs\n",
      "Summary:   The $M_{BH}$ - $\\sigma_{\\star}$ relation is considered a result of\n",
      "co-evolution between the host galaxies and their super-massive black holes. For\n",
      "elliptical-bulge hosting inactive galaxies, this relation is well established,\n",
      "but there is still a debate whether active galaxies follow the same relation.\n",
      "In this paper, we estimate black hole masses for a sample of 19 local luminous\n",
      "AGNs (LLAMA) in order to test their location on the $M_{BH}$ - $\\sigma_{\\star}$\n",
      "relation. Super-massive black hole masses ($M_{BH}$) were derived from the\n",
      "broad-line based relations for H$\\alpha$, H$\\beta$ and Pa$\\beta$ emission line\n",
      "profiles for the Type 1 AGNs. We compare the bulge stellar velocity dispersion\n",
      "($\\sigma_{\\star}$) as determined from the Ca II triplet (CaT) with the\n",
      "dispersion measured from the near-infrared CO (2-0) absorption features for\n",
      "each AGN and find them to be consistent with each other. We apply an extinction\n",
      "correction to the observed broad line fluxes and we correct the stellar\n",
      "velocity dispersion by an average rotation contribution as determined from\n",
      "spatially resolved stellar kinematic maps. The H$\\alpha$-based black hole\n",
      "masses of our sample of AGNs were estimated in the range 6.34 $\\leq$\n",
      "$\\log{M_{BH}}$ $\\leq$ 7.75 M$_\\odot$ and the $\\sigma_{\\star CaT}$ estimates\n",
      "range between 73 $\\leq$ $\\sigma_{\\star CaT}$ $\\leq$ 227 km s$^{-1}$. From the\n",
      "so-constructed $M_{BH}$ - $\\sigma_{\\star}$ relation for our Type 1 AGNs, we\n",
      "estimate the black hole masses for the Type 2 AGNs and the inactive galaxies in\n",
      "our sample. In conclusion, we find that our sample of local luminous AGNs is\n",
      "consistent with the $M_{BH}$ - $\\sigma_{\\star}$ relation of lower luminosity\n",
      "AGNs and inactive galaxies, after correcting for dust extinction and the\n",
      "rotational contribution to the stellar velocity dispersion.\n",
      "\n",
      "\n",
      "Title: Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video\n",
      "  Analytics Pipelines\n",
      "Summary:   The proliferation of camera-enabled devices and large video repositories has\n",
      "led to a diverse set of video analytics applications. These applications rely\n",
      "on video pipelines, represented as DAGs of operations, to transform videos,\n",
      "process extracted metadata, and answer questions like, \"Is this intersection\n",
      "congested?\" The latency and resource efficiency of pipelines can be optimized\n",
      "using configurable knobs for each operation (e.g., sampling rate, batch size,\n",
      "or type of hardware used). However, determining efficient configurations is\n",
      "challenging because (a) the configuration search space is exponentially large,\n",
      "and (b) the optimal configuration depends on users' desired latency and cost\n",
      "targets, (c) input video contents may exercise different paths in the DAG and\n",
      "produce a variable amount intermediate results. Existing video analytics and\n",
      "processing systems leave it to the users to manually configure operations and\n",
      "select hardware resources.\n",
      "  We present Llama: a heterogeneous and serverless framework for auto-tuning\n",
      "video pipelines. Given an end-to-end latency target, Llama optimizes for cost\n",
      "efficiency by (a) calculating a latency target for each operation invocation,\n",
      "and (b) dynamically running a cost-based optimizer to assign configurations\n",
      "across heterogeneous hardware that best meet the calculated per-invocation\n",
      "latency target. This makes the problem of auto-tuning large video pipelines\n",
      "tractable and allows us to handle input-dependent behavior, conditional\n",
      "branches in the DAG, and execution variability. We describe the algorithms in\n",
      "Llama and evaluate it on a cloud platform using serverless CPU and GPU\n",
      "resources. We show that compared to state-of-the-art cluster and serverless\n",
      "video analytics and processing systems, Llama achieves 7.8x lower latency and\n",
      "16x cost reduction on average.\n",
      "\n",
      "\n",
      "Title: LLAMA: Stellar populations in the nuclei of ultra hard X-ray selected\n",
      "  AGN and matched inactive galaxies\n",
      "Summary:   The relation between nuclear ($\\lesssim$ 50 pc) star formation and nuclear\n",
      "galactic activity is still elusive: theoretical models predict a link between\n",
      "the two, but it is unclear whether active galactic nuclei (AGNs) should appear\n",
      "at the same time, before or after nuclear star formation activity is ongoing.\n",
      "We present a study of this relation in a complete, volume-limited sample of\n",
      "nine of the most luminous ($\\log L_{\\rm 14-195 keV} > 10^{42.5}$ erg/s) local\n",
      "AGNs (the LLAMA sample), including a sample of 18 inactive control galaxies (6\n",
      "star-forming; 12 passive) that are matched by Hubble type, stellar mass (9.5\n",
      "$\\lesssim$ log M_star/M_sun $\\lesssim$ 10.5), inclination and distance. This\n",
      "allows us to calibrate our methods on the control sample and perform a\n",
      "differential analysis between the AGN and control samples. We perform stellar\n",
      "population synthesis on VLT/X-SHOOTER spectra in an aperture corresponding to a\n",
      "physical radius of $\\approx$ 150 pc. We find young ($\\lesssim$ 30 Myr) stellar\n",
      "populations in seven out of nine AGNs and in four out of six star-forming\n",
      "control galaxies. In the non-star-forming control population, in contrast, only\n",
      "two out of twelve galaxies show such a population. We further show that these\n",
      "young populations are not indicative of ongoing star-formation, providing\n",
      "evidence for models that see AGN activity as a consequence of nuclear star\n",
      "formation. Based on the similar nuclear star-formation histories of AGNs and\n",
      "star-forming control galaxies, we speculate that the latter may turn into the\n",
      "former for some fraction of their time. Under this assumption, and making use\n",
      "of the volume-completeness of our sample, we infer that the AGN phase lasts for\n",
      "about 5 % of the nuclear starburst phase.\n",
      "\n",
      "\n",
      "Title: Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A\n",
      "  Preliminary Study on Writing Assistance\n",
      "Summary:   Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered\n",
      "significant attention due to their exceptional capabilities in handling a\n",
      "diverse range of tasks. Recent studies demonstrate that open-sourced smaller\n",
      "foundational models, such as 7B-size LLaMA, can also display remarkable\n",
      "proficiency in tackling diverse tasks when fine-tuned using instruction-driven\n",
      "data. In this work, we investigate a practical problem setting where the\n",
      "primary focus is on one or a few particular tasks rather than general-purpose\n",
      "instruction following, and explore whether LLMs can be beneficial and further\n",
      "improved for such targeted scenarios. We choose the writing-assistant scenario\n",
      "as the testbed, which includes seven writing tasks. We collect training data\n",
      "for these tasks, reframe them in an instruction-following format, and\n",
      "subsequently refine the LLM, specifically LLaMA, via instruction tuning.\n",
      "Experimental results show that fine-tuning LLaMA on writing instruction data\n",
      "significantly improves its ability on writing tasks. We also conduct more\n",
      "experiments and analyses to offer insights for future work on effectively\n",
      "fine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion\n",
      "regarding the necessity of employing LLMs for only one targeted task, taking\n",
      "into account the efforts required for tuning and the resources consumed during\n",
      "deployment.\n",
      "\n",
      "\n",
      "Title: Music Understanding LLaMA: Advancing Text-to-Music Generation with\n",
      "  Question Answering and Captioning\n",
      "Summary:   Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity\n",
      "of large-scale publicly available music datasets with natural language\n",
      "captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),\n",
      "capable of answering music-related questions and generating captions for music\n",
      "files. Our model utilizes audio representations from a pretrained MERT model to\n",
      "extract music features. However, obtaining a suitable dataset for training the\n",
      "MU-LLaMA model remains challenging, as existing publicly accessible audio\n",
      "question answering datasets lack the necessary depth for open-ended music\n",
      "question answering. To fill this gap, we present a methodology for generating\n",
      "question-answer pairs from existing audio captioning datasets and introduce the\n",
      "MusicQA Dataset designed for answering open-ended music-related questions. The\n",
      "experiments demonstrate that the proposed MU-LLaMA model, trained on our\n",
      "designed MusicQA dataset, achieves outstanding performance in both music\n",
      "question answering and music caption generation across various metrics,\n",
      "outperforming current state-of-the-art (SOTA) models in both fields and\n",
      "offering a promising advancement in the T2M-Gen research field.\n",
      "\n",
      "\n",
      "Title: Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual\n",
      "  Predatory Chats and Abusive Texts\n",
      "Summary:   Detecting online sexual predatory behaviours and abusive language on social\n",
      "media platforms has become a critical area of research due to the growing\n",
      "concerns about online safety, especially for vulnerable populations such as\n",
      "children and adolescents. Researchers have been exploring various techniques\n",
      "and approaches to develop effective detection systems that can identify and\n",
      "mitigate these risks. Recent development of large language models (LLMs) has\n",
      "opened a new opportunity to address this problem more effectively. This paper\n",
      "proposes an approach to detection of online sexual predatory chats and abusive\n",
      "language using the open-source pretrained Llama 2 7B-parameter model, recently\n",
      "released by Meta GenAI. We fine-tune the LLM using datasets with different\n",
      "sizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).\n",
      "Based on the power of LLMs, our approach is generic and automated without a\n",
      "manual search for a synergy between feature extraction and classifier design\n",
      "steps like conventional methods in this domain. Experimental results show a\n",
      "strong performance of the proposed approach, which performs proficiently and\n",
      "consistently across three distinct datasets with five sets of experiments. This\n",
      "study's outcomes indicate that the proposed method can be implemented in\n",
      "real-world applications (even with non-English languages) for flagging sexual\n",
      "predators, offensive or toxic content, hate speech, and discriminatory language\n",
      "in online discussions and comments to maintain respectful internet or digital\n",
      "communities. Furthermore, it can be employed for solving text classification\n",
      "problems with other potential applications such as sentiment analysis, spam and\n",
      "phishing detection, sorting legal documents, fake news detection, language\n",
      "identification, user intent recognition, text-based product categorization,\n",
      "medical record analysis, and resume screening.\n",
      "\n",
      "\n",
      "Title: Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\n",
      "  Models that Follow Instructions\n",
      "Summary:   Training large language models to follow instructions makes them perform\n",
      "better on a wide range of tasks, generally becoming more helpful. However, a\n",
      "perfectly helpful model will follow even the most malicious instructions and\n",
      "readily generate harmful content. In this paper, we raise concerns over the\n",
      "safety of models that only emphasize helpfulness, not safety, in their\n",
      "instruction-tuning. We show that several popular instruction-tuned models are\n",
      "highly unsafe. Moreover, we show that adding just 3% safety examples (a few\n",
      "hundred demonstrations) in the training set when fine-tuning a model like LLaMA\n",
      "can substantially improve their safety. Our safety-tuning does not make models\n",
      "significantly less capable or helpful as measured by standard benchmarks.\n",
      "However, we do find a behavior of exaggerated safety, where too much\n",
      "safety-tuning makes models refuse to respond to reasonable prompts that\n",
      "superficially resemble unsafe ones. Our study sheds light on trade-offs in\n",
      "training LLMs to follow instructions and exhibit safe behavior.\n",
      "\n",
      "\n",
      "Title: Benchmarking quantized LLaMa-based models on the Brazilian Secondary\n",
      "  School Exam\n",
      "Summary:   Although Large Language Models (LLMs) represent a revolution in the way we\n",
      "interact with computers, allowing the construction of complex questions and the\n",
      "ability to reason over a sequence of statements, their use is restricted due to\n",
      "the need for dedicated hardware for execution. In this study, we evaluate the\n",
      "performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a\n",
      "quantization process and run on home hardware. The models considered were\n",
      "Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we\n",
      "developed a database containing 1,006 questions from the ENEM (Brazilian\n",
      "National Secondary School Exam). Our analysis revealed that the best performing\n",
      "models achieved an accuracy of approximately 46% for the original texts of the\n",
      "Portuguese questions and 49% on their English translations. In addition, we\n",
      "evaluated the computational efficiency of the models by measuring the time\n",
      "required for execution. On average, the 7 and 13 billion LLMs took\n",
      "approximately 20 and 50 seconds, respectively, to process the queries on a\n",
      "machine equipped with an AMD Ryzen 5 3600x processor\n",
      "\n",
      "\n",
      "Title: Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n",
      "  for Speech Recognition\n",
      "Summary:   We introduce a new cross-modal fusion technique designed for generative error\n",
      "correction in automatic speech recognition (ASR). Our methodology leverages\n",
      "both acoustic information and external linguistic representations to generate\n",
      "accurate speech transcription contexts. This marks a step towards a fresh\n",
      "paradigm in generative error correction within the realm of n-best hypotheses.\n",
      "Unlike the existing ranking-based rescoring methods, our approach adeptly uses\n",
      "distinct initialization techniques and parameter-efficient algorithms to boost\n",
      "ASR performance derived from pre-trained speech and text models. Through\n",
      "evaluation across diverse ASR datasets, we evaluate the stability and\n",
      "reproducibility of our fusion technique, demonstrating its improved word error\n",
      "rate relative (WERR) performance in comparison to n-best hypotheses by\n",
      "relatively 37.66%. To encourage future research, we have made our code and\n",
      "pre-trained models open source at\n",
      "https://github.com/Srijith-rkr/Whispering-LLaMA.\n",
      "\n",
      "\n",
      "Title: LLaMA Rider: Spurring Large Language Models to Explore the Open World\n",
      "Summary:   Recently, various studies have leveraged Large Language Models (LLMs) to help\n",
      "decision-making and planning in environments, and try to align the LLMs'\n",
      "knowledge with the world conditions. Nonetheless, the capacity of LLMs to\n",
      "continuously acquire environmental knowledge and adapt in an open world remains\n",
      "uncertain. In this paper, we propose an approach to spur LLMs to explore the\n",
      "open world, gather experiences, and learn to improve their task-solving\n",
      "capabilities. In this approach, a multi-round feedback-revision mechanism is\n",
      "utilized to encourage LLMs to actively select appropriate revision actions\n",
      "guided by feedback information from the environment. This facilitates\n",
      "exploration and enhances the model's performance. Besides, we integrate\n",
      "sub-task relabeling to assist LLMs in maintaining consistency in sub-task\n",
      "planning and help the model learn the combinatorial nature between tasks,\n",
      "enabling it to complete a wider range of tasks through training based on the\n",
      "acquired exploration experiences. By evaluation in Minecraft, an open-ended\n",
      "sandbox world, we demonstrate that our approach LLaMA-Rider enhances the\n",
      "efficiency of the LLM in exploring the environment, and effectively improves\n",
      "the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k\n",
      "instances of collected data, showing minimal training costs compared to the\n",
      "baseline using reinforcement learning.\n",
      "\n",
      "\n",
      "Title: Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via\n",
      "  Instruction Tuning with LITE\n",
      "Summary:   Large Language Models (LLMs) have achieved remarkable performance across a\n",
      "wide variety of natural language tasks; however, their large size makes their\n",
      "inference slow and computationally expensive. Focusing on this problem, we\n",
      "propose to instruction tune LLMs with additional explicit losses from the\n",
      "intermediate layers (LITE) and show that it enables these layers to acquire\n",
      "'good' generation ability without affecting the generation ability of the final\n",
      "layer. We perform 'dynamic confidence-based early exiting' at token level from\n",
      "the intermediate layers which improves the efficiency of text generation\n",
      "without compromising the quality of the generation. We conduct comprehensive\n",
      "experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and\n",
      "holistically evaluate on four different human-instruction test sets. We show\n",
      "that dynamic early exiting achieves consistent and considerable inference\n",
      "computation cost improvements (37.86% for 7B and 46.35% for 13B model) while\n",
      "maintaining the generation quality of the responses. We further conduct a\n",
      "thorough analysis of the results over several important aspects, such as\n",
      "comparing the semantic similarity of the outputs and dissecting the efficiency\n",
      "improvements by comparing the number of tokens generated in the output. In\n",
      "summary, our work contributes to improving the efficiency of LLM inference\n",
      "while maintaining the generation quality, a crucial step en route to enabling\n",
      "their widespread adoption.\n",
      "\n",
      "\n",
      "Title: LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B\n",
      "Summary:   AI developers often apply safety alignment procedures to prevent the misuse\n",
      "of their AI systems. For example, before Meta released Llama 2-Chat, a\n",
      "collection of instruction fine-tuned large language models, they invested\n",
      "heavily in safety training, incorporating extensive red-teaming and\n",
      "reinforcement learning from human feedback. However, it remains unclear how\n",
      "well safety training guards against model misuse when attackers have access to\n",
      "model weights. We explore the robustness of safety training in language models\n",
      "by subversively fine-tuning the public weights of Llama 2-Chat. We employ\n",
      "low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of\n",
      "less than $200 per model and using only one GPU, we successfully undo the\n",
      "safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,\n",
      "our fine-tuning technique significantly reduces the rate at which the model\n",
      "refuses to follow harmful instructions. We achieve a refusal rate below 1% for\n",
      "our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method\n",
      "retains general performance, which we validate by comparing our fine-tuned\n",
      "models against Llama 2-Chat across two benchmarks. Additionally, we present a\n",
      "selection of harmful outputs produced by our models. While there is\n",
      "considerable uncertainty about the scope of risks from current models, it is\n",
      "likely that future models will have significantly more dangerous capabilities,\n",
      "including the ability to hack into critical infrastructure, create dangerous\n",
      "bio-weapons, or autonomously replicate and adapt to new environments. We show\n",
      "that subversive fine-tuning is practical and effective, and hence argue that\n",
      "evaluating risks from fine-tuning should be a core part of risk assessments for\n",
      "releasing model weights.\n",
      "\n",
      "\n",
      "Title: Llamas Know What GPTs Don't Show: Surrogate Models for Confidence\n",
      "  Estimation\n",
      "Summary:   To maintain user trust, large language models (LLMs) should signal low\n",
      "confidence on examples where they are incorrect, instead of misleading the\n",
      "user. The standard approach of estimating confidence is to use the softmax\n",
      "probabilities of these models, but as of November 2023, state-of-the-art LLMs\n",
      "such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\n",
      "first study eliciting confidence linguistically -- asking an LLM for its\n",
      "confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\n",
      "averaged across 12 question-answering datasets -- 7% above a random baseline)\n",
      "but leaves room for improvement. We then explore using a surrogate confidence\n",
      "model -- using a model where we do have probabilities to evaluate the original\n",
      "model's confidence in a given question. Surprisingly, even though these\n",
      "probabilities come from a different and often weaker model, this method leads\n",
      "to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\n",
      "method composing linguistic confidences and surrogate model probabilities gives\n",
      "state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\n",
      "GPT-4).\n",
      "\n",
      "\n",
      "Title: HELLaMA: LLaMA-based Table to Text Generation by Highlighting the\n",
      "  Important Evidence\n",
      "Summary:   Large models have demonstrated significant progress across various domains,\n",
      "particularly in tasks related to text generation. In the domain of Table to\n",
      "Text, many Large Language Model (LLM)-based methods currently resort to\n",
      "modifying prompts to invoke public APIs, incurring potential costs and\n",
      "information leaks. With the advent of open-source large models, fine-tuning\n",
      "LLMs has become feasible. In this study, we conducted parameter-efficient\n",
      "fine-tuning on the LLaMA2 model. Distinguishing itself from previous\n",
      "fine-tuning-based table-to-text methods, our approach involves injecting\n",
      "reasoning information into the input by emphasizing table-specific row data.\n",
      "Our model consists of two modules: 1) a table reasoner that identifies relevant\n",
      "row evidence, and 2) a table summarizer that generates sentences based on the\n",
      "highlighted table. To facilitate this, we propose a search strategy to\n",
      "construct reasoning labels for training the table reasoner. On both the FetaQA\n",
      "and QTSumm datasets, our approach achieved state-of-the-art results.\n",
      "Additionally, we observed that highlighting input tables significantly enhances\n",
      "the model's performance and provides valuable interpretability.\n",
      "\n",
      "\n",
      "Title: SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion\n",
      "  Detection and Classification\n",
      "Summary:   Numerous studies have proved their effective strength in detecting Control\n",
      "Area Network (CAN) attacks. In the realm of understanding the human semantic\n",
      "space, transformer-based models have demonstrated remarkable effectiveness.\n",
      "Leveraging pre-trained transformers has become a common strategy in various\n",
      "language-related tasks, enabling these models to grasp human semantics more\n",
      "comprehensively. To delve into the adaptability evaluation on pre-trained\n",
      "models for CAN intrusion detection, we have developed two distinct models:\n",
      "CAN-SecureBERT and CAN-LLAMA2. Notably, our CAN-LLAMA2 model surpasses the\n",
      "state-of-the-art models by achieving an exceptional performance 0.999993 in\n",
      "terms of balanced accuracy, precision detection rate, F1 score, and a\n",
      "remarkably low false alarm rate of 3.10e-6. Impressively, the false alarm rate\n",
      "is 52 times smaller than that of the leading model, MTH-IDS (Multitiered Hybrid\n",
      "Intrusion Detection System). Our study underscores the promise of employing a\n",
      "Large Language Model as the foundational model, while incorporating adapters\n",
      "for other cybersecurity-related tasks and maintaining the model's inherent\n",
      "language-related capabilities.\n",
      "\n",
      "\n",
      "Title: Localizing Lying in Llama: Understanding Instructed Dishonesty on\n",
      "  True-False Questions Through Prompting, Probing, and Patching\n",
      "Summary:   Large language models (LLMs) demonstrate significant knowledge through their\n",
      "outputs, though it is often unclear whether false outputs are due to a lack of\n",
      "knowledge or dishonesty. In this paper, we investigate instructed dishonesty,\n",
      "wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt\n",
      "engineering to find which prompts best induce lying behavior, and then use\n",
      "mechanistic interpretability approaches to localize where in the network this\n",
      "behavior occurs. Using linear probing and activation patching, we localize five\n",
      "layers that appear especially important for lying. We then find just 46\n",
      "attention heads within these layers that enable us to causally intervene such\n",
      "that the lying model instead answers honestly. We show that these interventions\n",
      "work robustly across many prompts and dataset splits. Overall, our work\n",
      "contributes a greater understanding of dishonesty in LLMs so that we may hope\n",
      "to prevent it.\n",
      "\n",
      "\n",
      "Title: What Do Llamas Really Think? Revealing Preference Biases in Language\n",
      "  Model Representations\n",
      "Summary:   Do large language models (LLMs) exhibit sociodemographic biases, even when\n",
      "they decline to respond? To bypass their refusal to \"speak,\" we study this\n",
      "research question by probing contextualized embeddings and exploring whether\n",
      "this bias is encoded in its latent representations. We propose a logistic\n",
      "Bradley-Terry probe which predicts word pair preferences of LLMs from the\n",
      "words' hidden vectors. We first validate our probe on three pair preference\n",
      "tasks and thirteen LLMs, where we outperform the word embedding association\n",
      "test (WEAT), a standard approach in testing for implicit association, by a\n",
      "relative 27% in error rate. We also find that word pair preferences are best\n",
      "represented in the middle layers. Next, we transfer probes trained on harmless\n",
      "tasks (e.g., pick the larger number) to controversial ones (compare\n",
      "ethnicities) to examine biases in nationality, politics, religion, and gender.\n",
      "We observe substantial bias for all target classes: for instance, the Mistral\n",
      "model implicitly prefers Europe to Africa, Christianity to Judaism, and\n",
      "left-wing to right-wing politics, despite declining to answer. This suggests\n",
      "that instruction fine-tuning does not necessarily debias contextualized\n",
      "embeddings. Our codebase is at https://github.com/castorini/biasprobe.\n",
      "\n",
      "\n",
      "Title: Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations\n",
      "Summary:   We introduce Llama Guard, an LLM-based input-output safeguard model geared\n",
      "towards Human-AI conversation use cases. Our model incorporates a safety risk\n",
      "taxonomy, a valuable tool for categorizing a specific set of safety risks found\n",
      "in LLM prompts (i.e., prompt classification). This taxonomy is also\n",
      "instrumental in classifying the responses generated by LLMs to these prompts, a\n",
      "process we refer to as response classification. For the purpose of both prompt\n",
      "and response classification, we have meticulously gathered a dataset of high\n",
      "quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our\n",
      "collected dataset, albeit low in volume, demonstrates strong performance on\n",
      "existing benchmarks such as the OpenAI Moderation Evaluation dataset and\n",
      "ToxicChat, where its performance matches or exceeds that of currently available\n",
      "content moderation tools. Llama Guard functions as a language model, carrying\n",
      "out multi-class classification and generating binary decision scores.\n",
      "Furthermore, the instruction fine-tuning of Llama Guard allows for the\n",
      "customization of tasks and the adaptation of output formats. This feature\n",
      "enhances the model's capabilities, such as enabling the adjustment of taxonomy\n",
      "categories to align with specific use cases, and facilitating zero-shot or\n",
      "few-shot prompting with diverse taxonomies at the input. We are making Llama\n",
      "Guard model weights available and we encourage researchers to further develop\n",
      "and adapt them to meet the evolving needs of the community for AI safety.\n",
      "\n",
      "\n",
      "Title: Enhanced E-Commerce Attribute Extraction: Innovating with Decorative\n",
      "  Relation Correction and LLAMA 2.0-Based Annotation\n",
      "Summary:   The rapid proliferation of e-commerce platforms accentuates the need for\n",
      "advanced search and retrieval systems to foster a superior user experience.\n",
      "Central to this endeavor is the precise extraction of product attributes from\n",
      "customer queries, enabling refined search, comparison, and other crucial\n",
      "e-commerce functionalities. Unlike traditional Named Entity Recognition (NER)\n",
      "tasks, e-commerce queries present a unique challenge owing to the intrinsic\n",
      "decorative relationship between product types and attributes. In this study, we\n",
      "propose a pioneering framework that integrates BERT for classification, a\n",
      "Conditional Random Fields (CRFs) layer for attribute value extraction, and\n",
      "Large Language Models (LLMs) for data annotation, significantly advancing\n",
      "attribute recognition from customer inquiries. Our approach capitalizes on the\n",
      "robust representation learning of BERT, synergized with the sequence decoding\n",
      "prowess of CRFs, to adeptly identify and extract attribute values. We introduce\n",
      "a novel decorative relation correction mechanism to further refine the\n",
      "extraction process based on the nuanced relationships between product types and\n",
      "attributes inherent in e-commerce data. Employing LLMs, we annotate additional\n",
      "data to expand the model's grasp and coverage of diverse attributes. Our\n",
      "methodology is rigorously validated on various datasets, including Walmart,\n",
      "BestBuy's e-commerce NER dataset, and the CoNLL dataset, demonstrating\n",
      "substantial improvements in attribute recognition performance. Particularly,\n",
      "the model showcased promising results during a two-month deployment in\n",
      "Walmart's Sponsor Product Search, underscoring its practical utility and\n",
      "effectiveness.\n",
      "\n",
      "\n",
      "Title: LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian\n",
      "  Language\n",
      "Summary:   Large Language Models represent state-of-the-art linguistic models designed\n",
      "to equip computers with the ability to comprehend natural language. With its\n",
      "exceptional capacity to capture complex contextual relationships, the LLaMA\n",
      "(Large Language Model Meta AI) family represents a novel advancement in the\n",
      "field of natural language processing by releasing foundational models designed\n",
      "to improve the natural language understanding abilities of the transformer\n",
      "architecture thanks to their large amount of trainable parameters (7, 13, and\n",
      "70 billion parameters). In many natural language understanding tasks, these\n",
      "models obtain the same performances as private company models such as OpenAI\n",
      "Chat-GPT with the advantage to make publicly available weights and code for\n",
      "research and commercial uses. In this work, we investigate the possibility of\n",
      "Language Adaptation for LLaMA models, explicitly focusing on addressing the\n",
      "challenge of Italian Language coverage. Adopting an open science approach, we\n",
      "explore various tuning approaches to ensure a high-quality text generated in\n",
      "Italian suitable for common tasks in this underrepresented language in the\n",
      "original models' datasets. We aim to release effective text generation models\n",
      "with strong linguistic properties for many tasks that seem challenging using\n",
      "multilingual or general-purpose LLMs. By leveraging an open science philosophy,\n",
      "this study contributes to Language Adaptation strategies for the Italian\n",
      "language by introducing the novel LLaMAntino family of Italian LLMs.\n",
      "\n",
      "\n",
      "Title: The Lensed Lyman-Alpha MUSE Arcs Sample (LLAMAS) : I. Characterisation\n",
      "  of extended Lyman-alpha haloes and spatial offsets\n",
      "Summary:   We present the Lensed Lyman-Alpha MUSE Arcs Sample (LLAMAS) selected from\n",
      "MUSE and HST observations of 17 lensing clusters. The sample consists of 603\n",
      "continuum-faint (-23<M_UV<-14) lensed Lyman-alpha emitters (producing 959\n",
      "images) with spectroscopic redshifts between 2.9 and 6.7. Combining the power\n",
      "of cluster magnification with 3D spectroscopic observations, we are able to\n",
      "reveal the resolved morphological properties of 268 Lyman-alpha emitters. We\n",
      "use a forward modelling approach to model both Lyman-alpha and rest-frame UV\n",
      "continuum emission profiles in the source plane and measure spatial extent,\n",
      "ellipticity and spatial offsets between UV and Lyman-alpha emission. We find a\n",
      "significant correlation between UV continuum and Lyman-alpha spatial extent.\n",
      "Our characterization of the Lyman-alpha haloes indicates that the halo size is\n",
      "linked to the physical properties of the host galaxy (SFR, Lyman-alpha EW and\n",
      "Lyman-alpha line FWHM). We find that 48% of Lyman-alpha haloes are best-fitted\n",
      "by an elliptical emission distribution with a median axis ratio of q=0.48. We\n",
      "observe that 60% of galaxies detected both in UV and Lyman-alpha emission show\n",
      "a significant spatial offset (Delta). We measure a median offset of Delta= 0.58\n",
      "\\pm 0.14 kpc for the entire sample. By comparing the spatial offset values with\n",
      "the size of the UV component, we show that 40% of the offsets could be due to\n",
      "star-forming sub-structures in the UV component, while the larger offsets are\n",
      "more likely due to larger distance processes such as scattering effects inside\n",
      "the circumgalactic medium or emission from faint satellites or merging\n",
      "galaxies. Comparisons with a zoom-in radiative hydrodynamics simulation of a\n",
      "typical Lyman-alpha emitting galaxy show a good agreement with LLAMAS galaxies\n",
      "and indicate that bright star-formation clumps and satellite galaxies could\n",
      "produce a similar spatial offsets distribution. (abridged)\n",
      "\n",
      "\n",
      "Title: ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model\n",
      "  Meta-AI (LLaMA) Using Medical Domain Knowledge\n",
      "Summary:   The primary aim of this research was to address the limitations observed in\n",
      "the medical knowledge of prevalent large language models (LLMs) such as\n",
      "ChatGPT, by creating a specialized language model with enhanced accuracy in\n",
      "medical advice. We achieved this by adapting and refining the large language\n",
      "model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\n",
      "sourced from a widely used online medical consultation platform. These\n",
      "conversations were cleaned and anonymized to respect privacy concerns. In\n",
      "addition to the model refinement, we incorporated a self-directed information\n",
      "retrieval mechanism, allowing the model to access and utilize real-time\n",
      "information from online sources like Wikipedia and data from curated offline\n",
      "medical databases. The fine-tuning of the model with real-world patient-doctor\n",
      "interactions significantly improved the model's ability to understand patient\n",
      "needs and provide informed advice. By equipping the model with self-directed\n",
      "information retrieval from reliable online and offline sources, we observed\n",
      "substantial improvements in the accuracy of its responses. Our proposed\n",
      "ChatDoctor, represents a significant advancement in medical LLMs, demonstrating\n",
      "a significant improvement in understanding patient inquiries and providing\n",
      "accurate advice. Given the high stakes and low error tolerance in the medical\n",
      "field, such enhancements in providing accurate and reliable information are not\n",
      "only beneficial but essential.\n",
      "\n",
      "\n",
      "Title: Baby Llama: knowledge distillation from an ensemble of teachers trained\n",
      "  on a small dataset with no performance penalty\n",
      "Summary:   We present our submission to the BabyLM challenge, whose goal was to improve\n",
      "the sample efficiency of language models. We trained an ensemble consisting of\n",
      "a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word\n",
      "BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,\n",
      "which exceeds in performance both of its teachers as well as a similar model\n",
      "trained without distillation. This suggests that distillation can not only\n",
      "retain the full performance of the teacher model when the latter is trained on\n",
      "a sufficiently small dataset; it can exceed it, and lead to significantly\n",
      "better performance than direct training.\n",
      "\n",
      "\n",
      "Title: Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\n",
      "  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)\n",
      "Summary:   The rapid advancement of large language models (LLMs) has revolutionized\n",
      "natural language processing (NLP). While these models excel at understanding\n",
      "and generating human-like text, their widespread deployment can be\n",
      "prohibitively expensive. SortedNet is a recent training technique for enabling\n",
      "dynamic inference for deep neural networks. It leverages network modularity to\n",
      "create sub-models with varying computational loads, sorting them based on\n",
      "computation/accuracy characteristics in a nested manner. We extend SortedNet to\n",
      "generative NLP tasks, making large language models dynamic without any\n",
      "pretraining and by only replacing standard Supervised Fine-Tuning (SFT) with\n",
      "Sorted Fine-Tuning (SoFT) at the same costs. Our approach boosts model\n",
      "efficiency, eliminating the need for multiple models for various scenarios\n",
      "during inference. We show that using this approach, we are able to unlock the\n",
      "potential of intermediate layers of transformers in generating the target\n",
      "output. Our sub-models remain integral components of the original model,\n",
      "minimizing storage requirements and transition costs between different\n",
      "computational/latency budgets. By applying this approach on LLaMa 2 13B for\n",
      "tuning on the Stanford Alpaca dataset and comparing it to normal tuning and\n",
      "early exit via PandaLM benchmark, we show that Sorted Fine-Tuning can deliver\n",
      "models twice as fast as the original model while maintaining or exceeding\n",
      "performance.\n",
      "\n",
      "\n",
      "Title: ChatGPT, Llama, can you write my report? An experiment on assisted\n",
      "  digital forensics reports written using (Local) Large Language Models\n",
      "Summary:   Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or\n",
      "Llama, have advanced significantly, positioning them as valuable tools for\n",
      "digital forensics. While initial studies have explored the potential of ChatGPT\n",
      "in the context of investigations, the question of to what extent LLMs can\n",
      "assist the forensic report writing process remains unresolved. To answer the\n",
      "question, this article first examines forensic reports with the goal of\n",
      "generalization (e.g., finding the `average structure' of a report). We then\n",
      "evaluate the strengths and limitations of LLMs for generating the different\n",
      "parts of the forensic report using a case study. This work thus provides\n",
      "valuable insights into the automation of report writing, a critical facet of\n",
      "digital forensics investigations. We conclude that combined with thorough\n",
      "proofreading and corrections, LLMs may assist practitioners during the report\n",
      "writing process but at this point cannot replace them.\n",
      "\n",
      "\n",
      "Title: Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco\n",
      "  vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison\n",
      "Summary:   The success of ChatGPT has ignited an AI race, with researchers striving to\n",
      "develop new large language models (LLMs) that can match or surpass the language\n",
      "understanding and generation abilities of commercial ones. In recent times, a\n",
      "number of models have emerged, claiming performance near that of GPT-3.5 or\n",
      "GPT-4 through various instruction-tuning methods. As practitioners of\n",
      "Text-to-SQL parsing, we are grateful for their valuable contributions to\n",
      "open-source research. However, it is important to approach these claims with a\n",
      "sense of scrutiny and ascertain the actual effectiveness of these models.\n",
      "Therefore, we pit six popular large language models against each other,\n",
      "systematically evaluating their Text-to-SQL parsing capability on nine\n",
      "benchmark datasets with five different prompting strategies, covering both\n",
      "zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell\n",
      "significantly short of the performance achieved by closed-source models like\n",
      "GPT-3.5, highlighting the need for further work to bridge the performance gap\n",
      "between these models.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "papers = fetch_papers()\n",
    "if papers:\n",
    "    for paper in papers:\n",
    "        print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers saved to data/papers_data.txt\n"
     ]
    }
   ],
   "source": [
    "if papers:\n",
    "    # Specify the text file path\n",
    "    text_file_path = 'data/papers_data.txt'\n",
    "\n",
    "    # Open the text file in write mode\n",
    "    with open(text_file_path, 'w', encoding='utf-8') as textfile:\n",
    "        # Write each paper string to a new line\n",
    "        for paper in papers:\n",
    "            textfile.write(paper + '\\n\\n')\n",
    "\n",
    "    print(f\"Papers saved to {text_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas DataFrame from the papers_list\n",
    "df = pd.DataFrame(papers, columns=[\"PaperInfo\"])\n",
    "\n",
    "# Extract information from the \"PaperInfo\" column into separate columns (Title and Summary)\n",
    "df[['Title', 'Summary']] = df['PaperInfo'].str.extract(r'Title: (.*)\\nSummary: (.*)\\n')\n",
    "\n",
    "# Drop the original \"PaperInfo\" column\n",
    "df.drop('PaperInfo', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to data/papers_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Specify the CSV file path\n",
    "pkl_file_path = 'data/papers_data.pkl'\n",
    "df.to_pickle(pkl_file_path)\n",
    "print(f\"DataFrame saved to {pkl_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59 entries, 0 to 58\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    30 non-null     object\n",
      " 1   Summary  30 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30.000000\n",
       "mean     75.633333\n",
       "std       3.253469\n",
       "min      65.000000\n",
       "25%      75.000000\n",
       "50%      76.000000\n",
       "75%      78.000000\n",
       "max      79.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['Summary'].str.len()\n",
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(kind='box',vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda, RunnablePassthrough\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
