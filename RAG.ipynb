{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz+xGSWBTevCJeAbZv4yaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianacupp/QA_chatbot_llama2/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir5MkZMZzmlG",
        "outputId": "4e653979-5dcd-4371-cc6a-1d372539ef4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_spwS6mx0X0"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain openai faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain_experimental\n",
        "#!pip install \"langchain[docarray]\""
      ],
      "metadata": {
        "id": "izrVDCSYztr7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "D0Vg9uTxx4LX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_papers():\n",
        "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
        "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        root = ET.fromstring(data)\n",
        "\n",
        "        papers_list = []\n",
        "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
        "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
        "            paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
        "            papers_list.append(paper_info)\n",
        "\n",
        "        return papers_list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching papers: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "uCsFNGm6yHHV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "papers = fetch_papers()"
      ],
      "metadata": {
        "id": "qmq_DT7KyJKH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if papers:\n",
        "    for paper in papers:\n",
        "        print(paper)"
      ],
      "metadata": {
        "id": "Fys3a-vB0wrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(papers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdN_BK8KyKu5",
        "outputId": "e0c88c6d-d5bf-4270-cce7-b65e8730cf6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pandas DataFrame from the papers_list\n",
        "df = pd.DataFrame(papers, columns=[\"PaperInfo\"])\n",
        "\n",
        "# Extract information from the \"PaperInfo\" column into separate columns (Title and Summary)\n",
        "df[['Title', 'Summary']] = df['PaperInfo'].str.extract(r'Title: (.*)\\nSummary: (.*)\\n')\n",
        "\n",
        "# Drop the original \"PaperInfo\" column\n",
        "df.drop('PaperInfo', axis=1, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fDPM6QxyMx5",
        "outputId": "0a7e4837-223b-43a2-9e46-a50657ca6e1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59 entries, 0 to 58\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Title    30 non-null     object\n",
            " 1   Summary  30 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['length'] = df['Summary'].str.len()\n",
        "df['length'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJk7Qhv_yO8q",
        "outputId": "ac8612cb-27e4-4e63-de09-39752a4c91b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    30.000000\n",
              "mean     75.633333\n",
              "std       3.253469\n",
              "min      65.000000\n",
              "25%      75.000000\n",
              "50%      76.000000\n",
              "75%      78.000000\n",
              "max      79.000000\n",
              "Name: length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "JOOeZFqLySw3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Optionally, check that the environment variable was set correctly\n",
        "print(\"OPENAI_API_KEY has been set!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT6knfcpzHqK",
        "outputId": "424990cb-757a-4fb3-bc3d-8f495c490d23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: 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\n",
            "OPENAI_API_KEY has been set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_texts(papers, embedding=embeddings)"
      ],
      "metadata": {
        "id": "kM62YydF0a4u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "rt4NKBpCyoJk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n"
      ],
      "metadata": {
        "id": "hHkwwHGv1G1k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Bqk3W_qa1IXk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"For which tasks has Llama-2 already been used successfully?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "rjnOtDyT1QCl",
        "outputId": "daaa6bd5-841b-4cd6-aef4-d50719f42db7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the given context, Llama-2 has been successfully used for the following tasks:\\n\\n1. Dialogue use cases (Llama 2-Chat models)\\n2. Multitask analysis of financial news (text analysis, highlighting main points, summarizing, and extracting named entities with sentiments)\\n3. Writing assistance (improving ability on writing tasks)\\n4. Brazilian National Secondary School Exam (ENEM) question answering'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}