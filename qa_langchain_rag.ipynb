{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDF7ETcKob4g56Bgh0d/Oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianacupp/QA_chatbot_llama2/blob/main/qa_langchain_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir5MkZMZzmlG",
        "outputId": "4e653979-5dcd-4371-cc6a-1d372539ef4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_spwS6mx0X0"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain openai faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain_experimental\n",
        "#!pip install \"langchain[docarray]\""
      ],
      "metadata": {
        "id": "izrVDCSYztr7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "D0Vg9uTxx4LX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database creation"
      ],
      "metadata": {
        "id": "w_pMxi4d2Cxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_papers():\n",
        "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
        "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
        "    try:\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        root = ET.fromstring(data)\n",
        "\n",
        "        papers_list = []\n",
        "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
        "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
        "            paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
        "            papers_list.append(paper_info)\n",
        "\n",
        "        return papers_list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching papers: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "uCsFNGm6yHHV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "papers = fetch_papers()"
      ],
      "metadata": {
        "id": "qmq_DT7KyJKH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if papers:\n",
        "    for paper in papers:\n",
        "        print(paper)"
      ],
      "metadata": {
        "id": "Fys3a-vB0wrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(papers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdN_BK8KyKu5",
        "outputId": "e0c88c6d-d5bf-4270-cce7-b65e8730cf6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pandas DataFrame from the papers_list\n",
        "df = pd.DataFrame(papers, columns=[\"PaperInfo\"])\n",
        "\n",
        "# Extract information from the \"PaperInfo\" column into separate columns (Title and Summary)\n",
        "df[['Title', 'Summary']] = df['PaperInfo'].str.extract(r'Title: (.*)\\nSummary: (.*)\\n')\n",
        "\n",
        "# Drop the original \"PaperInfo\" column\n",
        "df.drop('PaperInfo', axis=1, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fDPM6QxyMx5",
        "outputId": "0a7e4837-223b-43a2-9e46-a50657ca6e1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59 entries, 0 to 58\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Title    30 non-null     object\n",
            " 1   Summary  30 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['length'] = df['Summary'].str.len()\n",
        "df['length'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJk7Qhv_yO8q",
        "outputId": "ac8612cb-27e4-4e63-de09-39752a4c91b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    30.000000\n",
              "mean     75.633333\n",
              "std       3.253469\n",
              "min      65.000000\n",
              "25%      75.000000\n",
              "50%      76.000000\n",
              "75%      78.000000\n",
              "max      79.000000\n",
              "Name: length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with langchain"
      ],
      "metadata": {
        "id": "upf6L7l62G4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "JOOeZFqLySw3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Optionally, check that the environment variable was set correctly\n",
        "print(\"OPENAI_API_KEY has been set!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT6knfcpzHqK",
        "outputId": "424990cb-757a-4fb3-bc3d-8f495c490d23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: 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\n",
            "OPENAI_API_KEY has been set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_texts(papers, embedding=embeddings)"
      ],
      "metadata": {
        "id": "kM62YydF0a4u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "rt4NKBpCyoJk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n"
      ],
      "metadata": {
        "id": "hHkwwHGv1G1k"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Bqk3W_qa1IXk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"For which tasks has Llama-2 already been used successfully?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rjnOtDyT1QCl",
        "outputId": "844d503b-c306-4c82-cf2f-844fd1e13217"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama-2 has been successfully used for the following tasks:\\n\\n1. Dialogue use cases, where Llama 2-Chat outperformed open-source chat models on most benchmarks.\\n2. Multitask analysis of financial news, including tasks such as analyzing a text from financial market perspectives, highlighting main points of a text, summarizing a text, and extracting named entities with appropriate sentiments.\\n3. Writing assistance, where Llama-2 was fine-tuned on writing instruction data and significantly improved its ability on writing tasks.\\n4. Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam, achieving an accuracy of approximately 46% for the original texts of the Portuguese questions and 49% on their English translations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with other questions\n",
        "chain.invoke(\"For which tasks has Llama-2 already been used successfully?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-uNzZFdb2Y0U",
        "outputId": "ac397437-49cf-42cb-880a-0311a2cf4501"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama-2 has already been used successfully for the following tasks:\\n1. Dialogue use cases (outperforming open-source chat models on most benchmarks)\\n2. Multitask analysis of financial news (analyzing text from financial market perspectives, highlighting main points, summarizing text, and extracting named entities with appropriate sentiments)\\n3. Writing assistance (improving ability on writing tasks through fine-tuning)\\n4. Benchmarking on the Brazilian Secondary School Exam (achieving accuracy of approximately 46% for original Portuguese questions and 49% for English translations)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What can you find out about the model structure of Llama-2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "g9QpsmHz3CT9",
        "outputId": "7a8e45ea-fef8-4072-bf65-417ca97a4e4f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the given context, we can find out that Llama-2 is a large language model (LLM) that has been pretrained and fine-tuned. It ranges in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, specifically Llama 2-Chat, are optimized for dialogue use cases. The document also mentions the approach to fine-tuning and safety improvements of Llama 2-Chat, indicating that the model structure has been modified and enhanced for specific purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "0wbmcgFB1s_G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"For which tasks has Llama-2 already been used successfully?\", \"language\": \"german\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PbtEpAAK1xGs",
        "outputId": "88321c60-ea20-4adf-8ae4-2a894c5e115e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama-2 wurde bereits erfolgreich für folgende Aufgaben verwendet:\\n- Dialognutzung\\n- Multitask-Analyse von Finanznachrichten\\n- Schreibunterstützung\\n- Benchmarking quantisierter LLaMa-basierter Modelle für die brasilianische Abschlussprüfung an weiterführenden Schulen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZV6uexf-14q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}